{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4 - Fine-tuning Faster R-CNN using Transfer Learning method\n",
        "\n",
        "We mannually annotated images with LabelImg and converted PIL images into a torch tensor. \n",
        "Pretrained Faster R-CNN ResNet-50-FPN was loaded, some layers were frozen and the pretrained model was trained on our small dataset of images. \n",
        "We downloaded pickle file with our model and tested it in Testing-TranferLearning-FasterRCNN.ipynb"
      ],
      "metadata": {
        "id": "YVrxK-ccTktM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing necessary libraries "
      ],
      "metadata": {
        "id": "4nqVCXCjTUpH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qDqSpbU3XXwG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torchvision\n",
        "import torch_snippets\n",
        "\n",
        "import os\n",
        "import time\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Converting manually annotated .xml files to a dictionary with bounding box coordinates of the objects on the image"
      ],
      "metadata": {
        "id": "aJ5x3pwUVBkz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZRRJu2eb1sU"
      },
      "outputs": [],
      "source": [
        "def xml_to_dict(xml_path):\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    return {'filename': xml_path,\n",
        "            \"image_width\": int(root.find(\"./size/width\").text),\n",
        "            \"image_height\": int(root.find(\"./size/height\").text),\n",
        "            \"image_channels\": int(root.find(\"./size/depth\").text),\n",
        "            \"label\": root.find(\"./object/name\").text,\n",
        "            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n",
        "            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n",
        "            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n",
        "            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Creating the ImageDataset class to preprocess the images from train, validation, and test datasets"
      ],
      "metadata": {
        "id": "rdyaJoFcUuCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icpms_dbe_NN"
      },
      "outputs": [],
      "source": [
        "# Convert human readable str label to int.\n",
        "label_dict = {\"keyboard\": 1, \"key\": 2, \"laptop\": 3, \"magnifying-glass\":4, \"mouse\":5, \"phone\":6 }\n",
        "# Convert label int to human readable str.\n",
        "reverse_label_dict = {1: \"keyboard\", 2: \"key\", 3: \"laptop\", 4: \"magnifying-glass\", 5: \"mouse\", 6: \"phone\"}\n",
        "\n",
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms = None):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            root: str\n",
        "                Path to the data folder.\n",
        "            transforms: Compose or list\n",
        "                Torchvision image transformations.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.files = [image[:-4] for image in sorted(os.listdir(root)) if image[-4:]=='.jpg']\n",
        "        self.label_dict = label_dict\n",
        "    def __getitem__(self, i):\n",
        "        # Load image from the hard disc.\n",
        "        img = PIL.Image.open(os.path.join(self.root, self.files[i] + \".jpg\")).convert(\"RGB\")\n",
        "        # Load annotation file from the hard disc.\n",
        "        ann = xml_to_dict(os.path.join(self.root, self.files[i] + \".xml\"))\n",
        "        # The target is given as a dict.\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.as_tensor([[ann[\"x1\"], \n",
        "                                            ann[\"y1\"], \n",
        "                                            ann[\"x2\"], \n",
        "                                            ann[\"y2\"]]], \n",
        "                                   dtype = torch.float32)\n",
        "        target[\"labels\"]=torch.as_tensor([label_dict[ann[\"label\"]]],\n",
        "                         dtype = torch.int64)\n",
        "        target[\"image_id\"] = torch.as_tensor(i)\n",
        "        # Apply any transforms to the data if required.\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        return img, target\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Creating the Compose class that transforms some torchvision images"
      ],
      "metadata": {
        "id": "4q4UtdSVVdL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gmvXEHNhbl8"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms.transforms as T\n",
        "class Compose:\n",
        "    \"\"\"\n",
        "    Composes several torchvision image transforms \n",
        "    as a sequence of transformations.\n",
        "    Inputs\n",
        "        transforms: list\n",
        "            List of torchvision image transformations.\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def __init__(self, transforms = []):\n",
        "        self.transforms = transforms\n",
        "    # __call__ sequentially performs the image transformations on\n",
        "    # the input image, and returns the augmented image.\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Creating the ToTensor class to transform images to torch tensor "
      ],
      "metadata": {
        "id": "r3Ae9onRWNSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfY5rpL3hmLL"
      },
      "outputs": [],
      "source": [
        "class ToTensor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Converts a PIL image into a torch tensor.\n",
        "    Inputs\n",
        "        image: PIL Image\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        image = F.convert_image_dtype(image)\n",
        "        return image, target\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "    \"\"\"\n",
        "    Randomly flips an image horizontally.\n",
        "    Inputs\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    Returns\n",
        "        image: tensor\n",
        "        target: dict\n",
        "    \"\"\"\n",
        "    def forward(self, image, target = None):\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                width, _ = F.get_image_size(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - \\\n",
        "                                     target[\"boxes\"][:, [2, 0]]\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Creating get_transform function ato use above created classes to transform images to torch tensor"
      ],
      "metadata": {
        "id": "6tNuRRnPW1u3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whc1Cchzh5dt"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    \"\"\"\n",
        "    Transforms a PIL Image into a torch tensor, and performs\n",
        "    random horizontal flipping of the image if training a model.\n",
        "    Inputs\n",
        "        train: bool\n",
        "            Flag indicating whether model training will occur.\n",
        "    Returns\n",
        "        compose: Compose\n",
        "            Composition of image transforms.\n",
        "    \"\"\"\n",
        "    transforms = []\n",
        "    # ToTensor is applied to all images.\n",
        "    transforms.append(ToTensor())\n",
        "    # The following transforms are applied only to the train set.\n",
        "    if train == True:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        # Other transforms can be added here later on.\n",
        "    return Compose(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Processing training and validation datasets"
      ],
      "metadata": {
        "id": "1D0yppGmXGgZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K1t2KSRiRkQ"
      },
      "outputs": [],
      "source": [
        "# Train dataset. \n",
        "# Set train = True to apply the training image transforms.\n",
        "train_ds = ImageDataset(\"./data/train\", get_transform(train = True))\n",
        "# Validation dataset.\n",
        "val_ds = ImageDataset(\"./data/val\", get_transform(train = False))\n",
        "# Test dataset.\n",
        "# test_ds = ImageDataset(\"./data/test2\", get_transform(train = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYwIdWC4kryg"
      },
      "outputs": [],
      "source": [
        "# Randomly shuffle all the data.\n",
        "indices = torch.randperm(len(train_ds)).tolist()\n",
        "# We split the entire data into 80/20 train-test splits. We further\n",
        "# split the train set into 80/20 train-validation splits. \n",
        "# Train dataset: 64% of the entire data, or 80% of 80%.\n",
        "train_ds = torch.utils.data.Subset(train_ds,\n",
        "           indices[:int(len(indices) * 0.64)])\n",
        "# Validation dataset: 16% of the entire data, or 20% of 80%.\n",
        "val_ds = torch.utils.data.Subset(val_ds, \n",
        "         indices[int(len(indices) * 0.64):int(len(indices) * 0.8)])\n",
        "# Test dataset: 20% of the entire data.\n",
        "# test_ds = torch.utils.data.Subset(test_ds, \n",
        "#           indices[int(len(indices) * 0.8):])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh5SF2XNk110"
      },
      "outputs": [],
      "source": [
        "# Collate image-target pairs into a tuple.\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "# Create the DataLoaders from the Datasets. \n",
        "train_dl = torch.utils.data.DataLoader(train_ds, \n",
        "                                 batch_size = 4, \n",
        "                                 shuffle = True, \n",
        "                        collate_fn = collate_fn)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, \n",
        "                             batch_size = 4, \n",
        "                            shuffle = False, \n",
        "                    collate_fn = collate_fn)\n",
        "# test_dl = torch.utils.data.DataLoader(test_ds, \n",
        "#                                batch_size = 4, \n",
        "#                               shuffle = False, \n",
        "#                       collate_fn = collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Loading pretrained fasterrcnn_resnet50_fpn model, freezing pretrained weights, replacing original classes"
      ],
      "metadata": {
        "id": "CGelKwTDXQeQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt1wX435lEOQ"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def get_object_detection_model(num_classes = 7, \n",
        "                               feature_extraction = True):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        num_classes: int\n",
        "            Number of classes to predict. Must include the \n",
        "            background which is class 0 by definition!\n",
        "        feature_extraction: bool\n",
        "            Flag indicating whether to freeze the pre-trained \n",
        "            weights. If set to True the pre-trained weights will be  \n",
        "            frozen and not be updated during.\n",
        "    Returns\n",
        "        model: FasterRCNN\n",
        "    \"\"\"\n",
        "    # Load the pretrained faster r-cnn model.\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "    # If True, the pre-trained weights will be frozen.\n",
        "    if feature_extraction == True:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "    # Replace the original 91 class top layer with a new layer\n",
        "    # tailored for num_classes.\n",
        "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats,\n",
        "                                                   num_classes)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Creating unbartch functions to unbatch batch of loaded data and train_batch function to train a batch of data"
      ],
      "metadata": {
        "id": "5VdSmcfkXx-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeOQ1Q48lX38"
      },
      "outputs": [],
      "source": [
        "def unbatch(batch, device):\n",
        "    \"\"\"\n",
        "    Unbatches a batch of data from the Dataloader.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        X: list\n",
        "            List of images.\n",
        "        y: list\n",
        "            List of dictionaries.\n",
        "    \"\"\"\n",
        "    X, y = batch\n",
        "    X = [x.to(device) for x in X]\n",
        "    y = [{k: v.to(device) for k, v in t.items()} for t in y]\n",
        "    return X, y\n",
        "def train_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Uses back propagation to train a model.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, losses\n",
        "@torch.no_grad()\n",
        "def validate_batch(batch, model, optimizer, device):\n",
        "    \"\"\"\n",
        "    Evaluates a model's loss value using validation data.\n",
        "    Inputs\n",
        "        batch: tuple\n",
        "            Tuple containing a batch from the Dataloader.\n",
        "        model: torch model\n",
        "        optimizer: torch optimizer\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        loss: float\n",
        "            Sum of the batch losses.\n",
        "        losses: dict\n",
        "            Dictionary containing the individual losses.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    return loss, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Creating train_fasterrcnn to train the model on our dataset"
      ],
      "metadata": {
        "id": "xHINiaeEYkAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvp1ythylmfT"
      },
      "outputs": [],
      "source": [
        "def train_fasterrcnn(model, \n",
        "                 optimizer, \n",
        "                  n_epochs, \n",
        "              train_loader, \n",
        "        test_loader = None, \n",
        "                log = None, \n",
        "               keys = None, \n",
        "            device = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Trains a FasterRCNN model using train and validation \n",
        "    Dataloaders over n_epochs. \n",
        "    Returns a Report on the training and validation losses.\n",
        "    Inputs\n",
        "        model: FasterRCNN\n",
        "        optimizer: torch optimizer\n",
        "        n_epochs: int\n",
        "            Number of epochs to train.\n",
        "        train_loader: DataLoader\n",
        "        test_loader: DataLoader\n",
        "        log: Record\n",
        "            torch_snippet Record to record training progress.\n",
        "        keys: list\n",
        "            List of strs containing the FasterRCNN loss names.\n",
        "        device: str\n",
        "            Indicates which device (CPU/GPU) to use.\n",
        "    Returns\n",
        "        log: Record\n",
        "            torch_snippet Record containing the training records.\n",
        "    \"\"\"\n",
        "    if log is None:\n",
        "        log = torch_snippets.Report(n_epochs)\n",
        "    if keys is None:\n",
        "        # FasterRCNN loss names.\n",
        "        keys = [\"loss_classifier\", \n",
        "                   \"loss_box_reg\", \n",
        "                \"loss_objectness\", \n",
        "               \"loss_rpn_box_reg\"]\n",
        "    model.to(device)\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        for ix, batch in enumerate(train_loader):\n",
        "            loss, losses = train_batch(batch, model, \n",
        "                                  optimizer, device)\n",
        "            # Record the current train loss.\n",
        "            pos = epoch + (ix + 1) / N\n",
        "            log.record(pos = pos, trn_loss = loss.item(), \n",
        "                       end = \"\\r\")\n",
        "        if test_loader is not None:\n",
        "            N = len(test_loader)\n",
        "            for ix, batch in enumerate(test_loader):\n",
        "                loss, losses = validate_batch(batch, model, \n",
        "                                         optimizer, device)\n",
        "                \n",
        "                # Record the current validation loss.\n",
        "                pos = epoch + (ix + 1) / N\n",
        "                log.record(pos = pos, val_loss = loss.item(), \n",
        "                           end = \"\\r\")\n",
        "    log.report_avgs(epoch + 1)\n",
        "    return log"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Training the model"
      ],
      "metadata": {
        "id": "7POiFub1Y0Rr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "70b3f32f5f074f0987f9401e06f2584d",
            "e41b2c2eb77840a7ab3fe54ed259500e",
            "a20abefeac594ed9935a6630d840f438",
            "c3be8af1b006447d8339420840c022bc",
            "b328bd80c0604fa9b573dc0d244a20dd",
            "3bccf7fb570f472abdfb025a8d3345af",
            "7bb45643b4014a529c55b29b20928fc3",
            "cba7d027f51f4ba7b872ab76f90ffda0",
            "01bda21fb9ad4f259253084c7124d3df",
            "b3a853b72b484e2cab17a0fab8ba2b1b",
            "9db280c34f3b465eb06f14906f73302f"
          ]
        },
        "id": "1oOSbLOJn8U9",
        "outputId": "d5fef086-a023-4abd-f36c-9c5cc0626570"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70b3f32f5f074f0987f9401e06f2584d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 10.000  val_loss: 0.211  trn_loss: 0.182  (11478.54s - 0.00s remaining)\n"
          ]
        }
      ],
      "source": [
        "# Create the faster rcnn model with 7 classes - mouse, laptop, key, keyboard, magnifying glass, phone and \n",
        "# background.\n",
        "model = get_object_detection_model(num_classes = 7,   \n",
        "                        feature_extraction = False)\n",
        "# Use the stochastic gradient descent optimizer.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                        lr = 0.005, \n",
        "                    momentum = 0.9, \n",
        "             weight_decay = 0.0005)\n",
        "# Train the model over 1 epoch.\n",
        "log = train_fasterrcnn(model = model, \n",
        "               optimizer = optimizer, \n",
        "                        n_epochs = 10,\n",
        "             train_loader = train_dl, \n",
        "                test_loader = val_dl,\n",
        "             log = None, keys = None,\n",
        "                     device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Downloaded the trained model in birary file format"
      ],
      "metadata": {
        "id": "T3ZVjcF7Y4Wu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MWKoLyFMx8Uv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save to file in the current working directory\n",
        "pkl_filename = \"fasterrcnn_model2.pkl\"\n",
        "with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(model, file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70b3f32f5f074f0987f9401e06f2584d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e41b2c2eb77840a7ab3fe54ed259500e",
              "IPY_MODEL_a20abefeac594ed9935a6630d840f438",
              "IPY_MODEL_c3be8af1b006447d8339420840c022bc"
            ],
            "layout": "IPY_MODEL_b328bd80c0604fa9b573dc0d244a20dd"
          }
        },
        "e41b2c2eb77840a7ab3fe54ed259500e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bccf7fb570f472abdfb025a8d3345af",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb45643b4014a529c55b29b20928fc3",
            "value": "100%"
          }
        },
        "a20abefeac594ed9935a6630d840f438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cba7d027f51f4ba7b872ab76f90ffda0",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01bda21fb9ad4f259253084c7124d3df",
            "value": 167502836
          }
        },
        "c3be8af1b006447d8339420840c022bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3a853b72b484e2cab17a0fab8ba2b1b",
            "placeholder": "​",
            "style": "IPY_MODEL_9db280c34f3b465eb06f14906f73302f",
            "value": " 160M/160M [00:00&lt;00:00, 191MB/s]"
          }
        },
        "b328bd80c0604fa9b573dc0d244a20dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bccf7fb570f472abdfb025a8d3345af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb45643b4014a529c55b29b20928fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cba7d027f51f4ba7b872ab76f90ffda0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01bda21fb9ad4f259253084c7124d3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3a853b72b484e2cab17a0fab8ba2b1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db280c34f3b465eb06f14906f73302f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}